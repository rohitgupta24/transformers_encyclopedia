What are Neural Networks ?
NN are very effective models to analyze complicated datatypes like images, text, audio, video. 
Explain Weights ? 
Explain Neurons ?
How it is different from Machine Learning ?

What are Transformers ?
A type of NN architecture.


Why Transformers were even needed ?
Attention was there always but allowing it to be worked parallely which helped in scaling up really produced the difference.
-Scale Efficiently
-Parallel processes
-Attention to Input meaning

Initially we used RNNs for text data. Couldn't be parallelized as they do translation sequentially.
Transformers can scale well and use plenty of GPUs to perform the tasks. Originally it was designed for Translation.They had done 3 special things :
        1.Positional Encoding : Instead of feeding word sequentiallly, slap a number on the word and feed it. Store the word order
                                into the word itself instead of doing it into the model. 
                                Train such data and model will understand the importance of word order.
        2.Attention : With the image, we can understand that on which word, model is giving importance while doing the translation for 
                      a particular word. And how should a model know to look to give more attention(importance) to which word, 
                      architecture has learned this by getting trained on tons of data. Attention was invented before this paper.
        3.Self-Attention : It's a twist in Attention. What if allow the model to know the underlying meaning of the input ?
                           It has been trained on tons of data that now model knows about the rules of grammar, language.
                           For this, models check the context by using surrounding words. 


Describe ChatGPT ?
ChatGPT = GPT + Reinforcement Learning, GPT = LLM + Transformers(seq to seq architecture with Encoders(Simulataneous Input) & Decoders(Sequential Output))
Language Models (Q&A, Summarize, Translate) have inherent capability to understand words and sentences, 
It understands probability sequence of distribution of words.


Foundation LLMs Models : BERT(smallest), GPT, LLaMA, BLOOM, FLAN-T5, PaLM. They take human written instructions(natural language or prompt) 
                         and perform tasks. Space that is available to the prompt is called the Context Window.

                         Output of model is called Completion. Act of using the model to generate text is called Inference.

                         Summary, Translation, Translation to Machine Code, Next Word Prediction, Information Retrieval(NER),
                         Connect LLMs to external API and do power interaction like, Is my Flight late?

                         The scale of foundation models grows from hundreds of millions of parameters to billions, even hundreds of billions, 
                         the subjective understanding of language that a model possesses also increases. This language understanding stored within the 
                         parameters of the model is what processes, reasons, and ultimately solves the tasks you give it, but it's also true that smaller 
                         models can be fine tuned to perform well on specific focused tasks.

Generating text is not New, RNNs were used for this. But with few words, prediction is hard,
If we increase the word than memory requirements increase exponentially. And even after that prediction is wrong as model needs more Information
to predict.
-- Homonyms
-- Syntactic Ambiguity (The teacher taught the student with the book)

The transformer architecture : This novel approach unlocked the progress in generative AI that we see today. 
It can be scaled efficiently to use multi-core GPUs, it can parallel process input data, making use of much larger 
training datasets, and crucially, it's able to learn to pay attention to the meaning of the words it's processing.

Learn the relevance and the context of every word.Attention Map illustartes the attention weights between each word.
ML models are just big statistical Calculators and they work with numbers not words.

Describe Tokenizer? Embedding ? Tokenizer should be same while generating text.

Each word has a token id and every token is mapped to a Vector. Original Paper : Vector Size = 512, if VS = 3, than word can be plot 
in 3d space and angle b/w words will define the relationship mathematically.  

Token Vectors + Positional Encoding = VEctors which is passed to Self Attention Layer.
Multi-Headed Attention : The number of attention heads included in the attention layer varies from model to model, but numbers in the range of 12-100 are common. 
The intuition here is that each self-attention head will learn a different aspect of language.

Softmax Layer : One single token will have higher probability score than the rest and that token(word) is chosen.